{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Taming Text\n",
    "\n",
    "## Deadline\n",
    "Thursday December 15, 2016 at 11:59PM\n",
    "\n",
    "## Important Notes\n",
    "* Make sure you push on GitHub your Notebook with all the cells already evaluated\n",
    "* Don't forget to add a textual description of your thought process, the assumptions you made, and the solution\n",
    "you plan to implement!\n",
    "* Please write all your comments in English, and use meaningful variable names in your code\n",
    "\n",
    "## Background\n",
    "In this homework you will explore a relatively large corpus of emails released in public during the\n",
    "[Hillary Clinton email controversy](https://en.wikipedia.org/wiki/Hillary_Clinton_email_controversy).\n",
    "You can find the corpus in the `hillary-clinton-emails` directory of this repository, while more detailed information \n",
    "about the [schema is available here](https://www.kaggle.com/kaggle/hillary-clinton-emails).\n",
    "\n",
    "## Assignment\n",
    "1. Generate a word cloud based on the raw corpus -- I recommend you to use the [Python word_cloud library](https://github.com/amueller/word_cloud).\n",
    "With the help of `nltk` (already available in your Anaconda environment), implement a standard text pre-processing \n",
    "pipeline (e.g., tokenization, stopword removal, stemming, etc.) and generate a new word cloud. Discuss briefly the pros and\n",
    "cons (if any) of the two word clouds you generated.\n",
    "\n",
    "2. Find all the mentions of world countries in the whole corpus, using the `pycountry` utility (*HINT*: remember that\n",
    "there will be different surface forms for the same country in the text, e.g., Switzerland, switzerland, CH, etc.)\n",
    "Perform sentiment analysis on every email message using the demo methods in the `nltk.sentiment.util` module. Aggregate \n",
    "the polarity information of all the emails by country, and plot a histogram (ordered and colored by polarity level)\n",
    "that summarizes the perception of the different countries. Repeat the aggregation + plotting steps using different demo\n",
    "methods from the sentiment analysis module -- can you find substantial differences?\n",
    "\n",
    "3. Using the `models.ldamodel` module from the [gensim library](https://radimrehurek.com/gensim/index.html), run topic\n",
    "modeling over the corpus. Explore different numbers of topics (varying from 5 to 50), and settle for the parameter which\n",
    "returns topics that you consider to be meaningful at first sight.\n",
    "\n",
    "4. *BONUS*: build the communication graph (unweighted and undirected) among the different email senders and recipients\n",
    "using the `NetworkX` library. Find communities in this graph with `community.best_partition(G)` method from the \n",
    "[community detection module](http://perso.crans.org/aynaud/communities/index.html). Print the most frequent 20 words used\n",
    "by the email authors of each community. Do these word lists look similar to what you've produced at step 3 with LDA?\n",
    "Can you identify clear discussion topics for each community? Discuss briefly the obtained results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
