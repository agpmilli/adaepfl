{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Taming Text\n",
    "\n",
    "## Deadline\n",
    "Thursday December 15, 2016 at 11:59PM\n",
    "\n",
    "## Important Notes\n",
    "* Make sure you push on GitHub your Notebook with all the cells already evaluated\n",
    "* Don't forget to add a textual description of your thought process, the assumptions you made, and the solution\n",
    "you plan to implement!\n",
    "* Please write all your comments in English, and use meaningful variable names in your code\n",
    "\n",
    "## Background\n",
    "In this homework you will explore a relatively large corpus of emails released in public during the\n",
    "[Hillary Clinton email controversy](https://en.wikipedia.org/wiki/Hillary_Clinton_email_controversy).\n",
    "You can find the corpus in the `hillary-clinton-emails` directory of this repository, while more detailed information \n",
    "about the [schema is available here](https://www.kaggle.com/kaggle/hillary-clinton-emails).\n",
    "\n",
    "## Assignment\n",
    "1. Generate a word cloud based on the raw corpus -- I recommend you to use the [Python word_cloud library](https://github.com/amueller/word_cloud).\n",
    "With the help of `nltk` (already available in your Anaconda environment), implement a standard text pre-processing \n",
    "pipeline (e.g., tokenization, stopword removal, stemming, etc.) and generate a new word cloud. Discuss briefly the pros and\n",
    "cons (if any) of the two word clouds you generated.\n",
    "\n",
    "2. Find all the mentions of world countries in the whole corpus, using the `pycountry` utility (*HINT*: remember that\n",
    "there will be different surface forms for the same country in the text, e.g., Switzerland, switzerland, CH, etc.)\n",
    "Perform sentiment analysis on every email message using the demo methods in the `nltk.sentiment.util` module. Aggregate \n",
    "the polarity information of all the emails by country, and plot a histogram (ordered and colored by polarity level)\n",
    "that summarizes the perception of the different countries. Repeat the aggregation + plotting steps using different demo\n",
    "methods from the sentiment analysis module -- can you find substantial differences?\n",
    "\n",
    "3. Using the `models.ldamodel` module from the [gensim library](https://radimrehurek.com/gensim/index.html), run topic\n",
    "modeling over the corpus. Explore different numbers of topics (varying from 5 to 50), and settle for the parameter which\n",
    "returns topics that you consider to be meaningful at first sight.\n",
    "\n",
    "4. *BONUS*: build the communication graph (unweighted and undirected) among the different email senders and recipients\n",
    "using the `NetworkX` library. Find communities in this graph with `community.best_partition(G)` method from the \n",
    "[community detection module](http://perso.crans.org/aynaud/communities/index.html). Print the most frequent 20 words used\n",
    "by the email authors of each community. Do these word lists look similar to what you've produced at step 3 with LDA?\n",
    "Can you identify clear discussion topics for each community? Discuss briefly the obtained results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import nltk\n",
    "import string\n",
    "import collections\n",
    "nltk.download()\n",
    "from os import path\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Generate a word cloud based on emails' content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load email with subjects, already extracted text and raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raws = pd.read_csv('hillary-clinton-emails/Emails.csv',usecols=['ExtractedSubject','ExtractedBodyText','RawText'])\n",
    "raws.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see some NaN extractedSubject/extractedBodytext. After visualizing RawText, a majority of the cases can be explained (e.g no subject, email forwarding,..)\n",
    "We decide to trust the latter and drop RawText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raws.drop(['RawText'], axis= 1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def concat_subj_txt(row):\n",
    "    subj = '' if pd.isnull(row.ExtractedSubject) else row.ExtractedSubject\n",
    "    text = '' if pd.isnull(row.ExtractedBodyText) else row.ExtractedBodyText\n",
    "    return subj+text\n",
    "\n",
    "body = pd.DataFrame()\n",
    "body['text'] = raws.apply(concat_subj_txt, axis=1)\n",
    "body.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. dumb word cloud with concat of all cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = ' '.join(body['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Naive Wordcloud on raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classic_cloud(text):\n",
    "    wordcloud = WordCloud().generate(text)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classic_cloud(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad. Let's try something more stylish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def img_cloud(text, img):\n",
    "    img = Image.open(img)\n",
    "    img = img.resize((980,1080), Image.ANTIALIAS)\n",
    "    hcmask = np.array(img)\n",
    "    wordcloud = WordCloud(background_color=\"white\", max_words=2000, mask=hcmask).generate(text)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_cloud(text, 'hc.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordCloud on clean/preprocessed data\n",
    "Observe frequent words and remove the ones that are frequent and unwanted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "body_tokenized = [i for i in word_tokenize(text.lower())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "body_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "stop.update(string.punctuation) #Remove ponctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "body_tokenized = [i for i in word_tokenize(text.lower()) if i not in stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing the 'english' stopwords and the punctuaction, we verify what we still need to remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counter=collections.Counter(body_tokenized)\n",
    "print(counter.most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove the unwanted words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop.update([\"'s\", \"''\", \"``\", 'pm', 'fw', \"n't\", '--']) #Remove other unwanted characters and words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "body_tokenized = [i for i in word_tokenize(text.lower()) if i not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classic_cloud(' '.join(body_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
