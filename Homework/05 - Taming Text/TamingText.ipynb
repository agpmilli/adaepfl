{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Taming Text\n",
    "\n",
    "## Deadline\n",
    "Thursday December 15, 2016 at 11:59PM\n",
    "\n",
    "## Important Notes\n",
    "* Make sure you push on GitHub your Notebook with all the cells already evaluated\n",
    "* Don't forget to add a textual description of your thought process, the assumptions you made, and the solution\n",
    "you plan to implement!\n",
    "* Please write all your comments in English, and use meaningful variable names in your code\n",
    "\n",
    "## Background\n",
    "In this homework you will explore a relatively large corpus of emails released in public during the\n",
    "[Hillary Clinton email controversy](https://en.wikipedia.org/wiki/Hillary_Clinton_email_controversy).\n",
    "You can find the corpus in the `hillary-clinton-emails` directory of this repository, while more detailed information \n",
    "about the [schema is available here](https://www.kaggle.com/kaggle/hillary-clinton-emails).\n",
    "\n",
    "## Assignment\n",
    "1. Generate a word cloud based on the raw corpus -- I recommend you to use the [Python word_cloud library](https://github.com/amueller/word_cloud).\n",
    "With the help of `nltk` (already available in your Anaconda environment), implement a standard text pre-processing \n",
    "pipeline (e.g., tokenization, stopword removal, stemming, etc.) and generate a new word cloud. Discuss briefly the pros and\n",
    "cons (if any) of the two word clouds you generated.\n",
    "\n",
    "2. Find all the mentions of world countries in the whole corpus, using the `pycountry` utility (*HINT*: remember that\n",
    "there will be different surface forms for the same country in the text, e.g., Switzerland, switzerland, CH, etc.)\n",
    "Perform sentiment analysis on every email message using the demo methods in the `nltk.sentiment.util` module. Aggregate \n",
    "the polarity information of all the emails by country, and plot a histogram (ordered and colored by polarity level)\n",
    "that summarizes the perception of the different countries. Repeat the aggregation + plotting steps using different demo\n",
    "methods from the sentiment analysis module -- can you find substantial differences?\n",
    "\n",
    "3. Using the `models.ldamodel` module from the [gensim library](https://radimrehurek.com/gensim/index.html), run topic\n",
    "modeling over the corpus. Explore different numbers of topics (varying from 5 to 50), and settle for the parameter which\n",
    "returns topics that you consider to be meaningful at first sight.\n",
    "\n",
    "4. *BONUS*: build the communication graph (unweighted and undirected) among the different email senders and recipients\n",
    "using the `NetworkX` library. Find communities in this graph with `community.best_partition(G)` method from the \n",
    "[community detection module](http://perso.crans.org/aynaud/communities/index.html). Print the most frequent 20 words used\n",
    "by the email authors of each community. Do these word lists look similar to what you've produced at step 3 with LDA?\n",
    "Can you identify clear discussion topics for each community? Discuss briefly the obtained results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#required imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import string\n",
    "import collections\n",
    "import re\n",
    "import pycountry\n",
    "from os import path\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import bigrams\n",
    "from nltk import ngrams\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.tokenize import treebank\n",
    "from pylab import rcParams\n",
    "import sentiment as senth\n",
    "from gensim import models, corpora\n",
    "\n",
    "\n",
    "#local helper file import\n",
    "import nlp_helper as nlph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Generate a word cloud based on emails' content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load email with subjects, already extracted text and raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raws = pd.read_csv('hillary-clinton-emails/Emails.csv',usecols=['ExtractedSubject','ExtractedBodyText','RawText'])\n",
    "raws.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see some NaN extractedSubject/extractedBodytext. After visualizing RawText, a majority of the cases can be explained (e.g no subject, email forwarding,..)\n",
    "We decide to trust the latter and drop RawText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raws.drop(['RawText'], axis= 1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "body = pd.DataFrame()\n",
    "body['text'] = raws.apply(nlph.concat_subj_txt, axis=1)\n",
    "body.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. dumb word cloud with concat of all cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = ' '.join(body['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Naive Wordcloud on raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nlph.classic_cloud(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad. Let's try something more stylish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nlph.img_cloud(text, 'hc.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. WordCloud on clean/preprocessed data\n",
    "Observe frequent words and remove the ones that are frequent and unwanted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "body_tokenized = [i for i in word_tokenize(text.lower())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "stop.update(string.punctuation) #Remove ponctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "body_tokenized = [i for i in word_tokenize(text.lower()) if i not in stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing the 'english' stopwords and the punctuaction, we verify what we still need to remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counter=collections.Counter(body_tokenized)\n",
    "print(counter.most_common()[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove the unwanted words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop.update(['nan', '\\'s', '--', '``', 'w', 'fw', 'n\\'t', '\\'m', 'also', 'thx', 'fyi', 'pls', '\\'\\'', '-', '—', 'pm', '•']) #Remove other unwanted characters and words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "body_tokenized = [i for i in word_tokenize(text.lower()) if i not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nlph.classic_cloud(' '.join(body_tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "If we print body_tokenized, we can still see some unwanted strings like dots ('..', '...'), dates and url. Since they are quite rare, we could ignore their existence for the only purpose of drawing the workdcloud which only considers the most frequent tokens. However, we will need a clean data later on. We thus decide to remove the latters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(body_tokenized))\n",
    "r = re.compile('[.]{1,3}$')\n",
    "dots = set(filter(r.match, body_tokenized))\n",
    "body_tokenized = [token for token in body_tokenized if token not in dots]\n",
    "print(len(body_tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We thus removed 477 tokens. Now, let's remove the url and files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "is_file = lambda x: x.endswith('.docx')\n",
    "is_url = lambda x: x.startswith('htte/') or x.startswith('http/')\n",
    "body_tokenized = [token for token in body_tokenized if not is_file(token) and not is_url(token)]\n",
    "print(len(body_tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is still not perfect since we still can find some unwanted symbols, useless dates/times and numbers.\n",
    "Let's now try to apply some normalization techniques on our data.\n",
    "Two classic methods are Stemming and Lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "stemmed = [porter.stem(token) for token in body_tokenized]\n",
    "lemmatized = [wnl.lemmatize(token) for token in body_tokenized]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compute the word frequencies and look in their respective top10 for some potential differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_stemm = collections.Counter(stemmed)\n",
    "print(\"################## Stemmed Tokens ####################\")\n",
    "print(count_stemm.most_common()[:20])\n",
    "\n",
    "count_lemm = collections.Counter(lemmatized)\n",
    "print(\"################## Lemmatized Tokens #################\")\n",
    "print(count_lemm.most_common()[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----> Talk about differences <-------\n",
    "We print the lemmatized wordcloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nlph.img_cloud(' '.join(lemmatized), 'hc.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much difference with not stemmed/lemmatized tokens.\n",
    "\n",
    "---------> TODO: discuss pros/cons of each clouds<----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Test: Frequency Analysis of bigrams\n",
    "\n",
    "We see plenty of verbs and adjectives that lack some context to become meaningful. Let's start by creating bigrams from the \"clean\" text we obtained before. We could then check the top bigrams in term of frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens_bigram = bigrams(lemmatized)\n",
    "count_bigram= collections.Counter(tokens_bigram)\n",
    "print(count_bigram.most_common()[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe some interesting pairs of words coming up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Identification of countries and sentiment analysis per email. Aggregation of perceptions on countries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.1. Detect Countries\n",
    "We now want to extract the mentions of countries in the text of each email.\n",
    "We will start with our DataFrame \"body\" which contained the aggregated text of the two fields (subject + body text). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "geo_body = body.copy()\n",
    "geo_body.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to extract the country names in each of our rows (=email). We thus create different helper lists from the pycountry set of countries. They will help us returning a country name from its alpha_2, alpha_3 and lower case form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a2_name = {}\n",
    "a3_name = {}\n",
    "for c in pycountry.countries:\n",
    "    a2_name[c.alpha_2] = c.name\n",
    "    a3_name[c.alpha_3] = c.name\n",
    "    \n",
    "names = [c.name for c in pycountry.countries]\n",
    "lowers = map(lambda s: s.lower(), names)\n",
    "lower_to_name = {}\n",
    "for c in pycountry.countries:\n",
    "    lower_to_name[c.name.lower()] = c.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tokenize our rows and clean them a little bit by removing some punctuation tokens. We do not remove stop words since a country could be composed of multiple words including a stop word.\n",
    "We also need to remove tokens like 'pm', 'fw' and others which could be detected as country ids (e.g pm <=> saint pierre and miquelon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tokenization\n",
    "geo_body['text'] = geo_body.apply(lambda row: word_tokenize(row.text) , axis=1)\n",
    "#punctuation removal\n",
    "geo_body['text'] = geo_body.apply(lambda row: [i for i in row.text if i not in string.punctuation], axis=1)\n",
    "#bad ids\n",
    "geo_body['text'] = geo_body.apply(lambda row: [i for i in row.text if i not in ['PM', 'RE', 'AM', 'TO', 'FM', 'NO', 'AND']], axis=1)\n",
    "\n",
    "geo_body.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "geo_body['countries'] = geo_body.apply(nlph.extract_country, axis=1)\n",
    "geo_body.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some first country results. However we need to go further since we are overlooking countries in two different scenarios:\n",
    "- a country could be composed of multiple tokens (e.g united states)\n",
    "- a country can be represented by a city (e.g Cairo, row 3)\n",
    "\n",
    "We first create new columns containing the bigrams and 3-grams of each text and then search again for countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def composed_token(grams):\n",
    "    return list(map(lambda x: ' '.join(x), grams))\n",
    "        \n",
    "\n",
    "geo_body['bigrams'] = geo_body.apply(lambda r: composed_token(bigrams(r.text)), axis=1)\n",
    "geo_body['trigrams'] = geo_body.apply(lambda r: composed_token(ngrams(r.text,3)), axis=1)\n",
    "geo_body.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "updates countries and add the ones found through bigrams and trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "geo_body['countries'] = geo_body.apply(nlph.bigram_search, axis=1)\n",
    "geo_body['countries'] = geo_body.apply(nlph.trigram_search, axis=1)\n",
    "geo_body.drop(['bigrams', 'trigrams'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "geo_body.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtained for instance the country United states and others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still lack the cases where cities are mentionned in an email without the country. \n",
    "We thus use a named-entity recognition classifier (stanford-nltk). We then query for the country of the locations obtained.\n",
    "\n",
    "->__SEE NOTEBOOK: \"__country_detection_stanfordNER.ipynb\"\n",
    "\n",
    "It is computationally expensive (~4-5h for a full location detection). \n",
    "TO RUN AND INSERT HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.2. Sentiment Analysis\n",
    "\n",
    "As seen in the demos of the following page http://www.nltk.org/_modules/nltk/sentiment/util.html, there are many ways to compute a sentiment polarity or subjectivity of a given document.\n",
    "\n",
    "The first solutions could be to train a classifier on an existing corpus and then classify our emails (see demo_twitter, demo_movies). However, those datasets are very domain specific. \n",
    "\n",
    "We thus decided to use two algorithms namely the Vader approach and the Liu Hu lexicon method.\n",
    "Those two methods share the property that they do not need any training beforehand in order to classify a given sentence/text.\n",
    "\n",
    "\n",
    "We first define the two row classification methods:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vote(pos, neg):\n",
    "    polarity = 0\n",
    "    if pos > neg:\n",
    "        polarity = 1\n",
    "    elif pos < neg:\n",
    "        polarity = -1\n",
    "    return polarity\n",
    "\n",
    "def vader_classify(row):\n",
    "    vader_analyzer = SentimentIntensityAnalyzer()\n",
    "    polarity = vader_analyzer.polarity_scores(row.text)\n",
    "    pos = polarity['pos']\n",
    "    neg = polarity['neg']\n",
    "    return vote(pos, neg)\n",
    "\n",
    "def liu_hu_classify(row):\n",
    "    tokenizer = treebank.TreebankWordTokenizer()\n",
    "    pos_words = 0\n",
    "    neg_words = 0\n",
    "    tokenized_sent = [word.lower() for word in tokenizer.tokenize(row.text)]\n",
    "    for word in tokenized_sent:\n",
    "        if word in opinion_lexicon.positive():\n",
    "            pos_words += 1\n",
    "        elif word in opinion_lexicon.negative():\n",
    "            neg_words += 1\n",
    "        else:\n",
    "            continue\n",
    "    return vote(pos_words, neg_words)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a new dataframe including a column for each sentiment analysis method output per row email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if 0:\n",
    "    sent_body = body.copy()\n",
    "    sent_body['Vader'] = sent_body.apply(vader_classify, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the run is very computationally expensive we save the obtained dataframe.\n",
    "\n",
    "|text|vader polarity|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if 0:\n",
    "    sent_body.to_csv('sentiment_vader.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same for the Liu Hu method.\n",
    "We do not run this sentiment analysis for now. it is very expensive and take too much time. We will try to run it later on and compare the obtained results with the vader outputs.\n",
    " \n",
    "|text|vader polarity|liu hu polarity|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if 0:\n",
    "    sent_body['LiuHu'] = sent_body.apply(liu_hu_classify, axis=1)\n",
    "    sent_body.to_csv('sentiment_vader_liu_hu.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We load our csv containing the table with vader sentiment analysis and concatenate it with the table containing the detected countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent_body = pd.read_csv('sentiment_vader.csv', index_col=0)\n",
    "sent_body.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "geo_sent = pd.concat([geo_body, sent_body], axis=1)\n",
    "geo_sent.drop(['text'], axis=1, inplace=True)\n",
    "geo_sent.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now desaggregate the countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "country_sentiment = pd.DataFrame(columns=['Country', 'Sentiment'])\n",
    "\n",
    "def desaggregate(row):\n",
    "    if len(row.countries) != 0:\n",
    "        for c in row.countries:\n",
    "            country_sentiment.loc[len(country_sentiment)] = [c, row.Vader]\n",
    "\n",
    "\n",
    "geo_sent.apply(desaggregate, axis=1)\n",
    "country_sentiment.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "country_sentiment['Frequency'] = 1\n",
    "country_sentiment = country_sentiment.groupby('Country').sum()\n",
    "country_sentiment['Sentiment'] = country_sentiment['Sentiment'] / country_sentiment['Frequency']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "country_sentiment = country_sentiment.sort_values(by='Frequency', ascending=False)\n",
    "country_sentiment.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now plot the country frequencies as well as a sentiment indicator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Credits to Stack Overflow :\n",
    "# http://stackoverflow.com/questions/31313606/pyplot-matplotlib-bar-chart-with-fill-color-depending-on-value\n",
    "from matplotlib import cm\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "# Set up colors : red to green\n",
    "y = np.array(country_sentiment['Sentiment'][:50])\n",
    "colors = cm.RdYlGn(y / float(max(y)))\n",
    "plot = plt.scatter(y, y, c=y, cmap = 'RdYlGn')\n",
    "plt.clf()\n",
    "clb = plt.colorbar(plot)\n",
    "clb.ax.set_title(\"Sentiment\")\n",
    "\n",
    "# Display bar plot : country frequency vs. country name, with color indicating polarity score\n",
    "plt.bar(range(50), country_sentiment['Frequency'][:50], align='center', tick_label=country_sentiment.index[:50], color=colors)\n",
    "plt.xticks(rotation=70, ha='right')\n",
    "plt.xlabel(\"Country\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3. Topic Modeling on the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start from the extracted subjects and body texts in the emails, concat them and remove the stopwords and ponctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raws = pd.read_csv('hillary-clinton-emails/Emails.csv',usecols=['ExtractedSubject','ExtractedBodyText'])\n",
    "body = pd.DataFrame()\n",
    "body['text'] = raws.apply(nlph.concat_subj_txt, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "stop.update(string.punctuation) #Remove ponctuation\n",
    "stop.update(['nan', '\\'s', '--', '``', 'w', 'fw', 'n\\'t', '\\'m', 'also', 'thx', 'fyi', 'pls', '\\'\\'', '-', '—', 'pm', '•'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "body_tokenized = [[i for i in word_tokenize(text.lower())if i not in stop] for text in body['text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dictionary and the corpus using the tokenized body of the emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(body_tokenized)\n",
    "corpus = [dictionary.doc2bow(text) for text in body_tokenized]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then build the LDA Model with number of topics between 5 to 50 and print each topic with 7 words to compare what number of topic returns meaningful topics at first sight  :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(5,51,5):\n",
    "    lda = models.LdaModel(corpus, id2word=dictionary, num_topics=i)\n",
    "    print(\"Number of topics : \", i , \" / Number of words per topic : 7 \\n\")\n",
    "    model = lda.show_topics(num_topics = i, num_words = 7, formatted=False)\n",
    "    i = 0\n",
    "    for topic in model:\n",
    "        wordlist = topic[1]\n",
    "        i+=1\n",
    "        print(\"Topic \" + str(i) + \" has words :\", end=\" \")\n",
    "        for word, prob in wordlist:\n",
    "            print(word, end=\", \")\n",
    "        print(\"\")\n",
    "    print(\"\\n-------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can say that if the number of topics is appropriate the words in a topic seems related. The number of topics is too small when a topic contains words that could be split in two different topics. On the other hand the number of topics is too high when the words defining one topic are split in multiple topics. \n",
    "\n",
    "In our case, we find that 20 topics gives meaningful results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
